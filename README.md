<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>README.md</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="artificial-neural-network-ann">Artificial Neural Network (ANN)</h1>
<p>In this project a multilayer artificial neural network for binary classification is implemented <em>from scratch</em> in <code>Python</code>.</p>
<h2 id="data-sets">Data Sets</h2>
<p>Information on the datasets can be found in the comment blocks of the file <code>preprocess_data.py</code>.</p>
<h2 id="network-architecture">Network Architecture</h2>
<p>The multi-layer neural network has a single hidden layer with full feed-forward interconnection (neurons in one layer are connected to every neuron in the next); this enables the learning of nonlinear decision boundaries.</p>
<p>The number of neurons in the hidden layer, <img src="https://latex.codecogs.com/gif.latex?h" alt="Equation">, has been fixed according to the dimension of the input layer, i.e., the number of predictor variables in the specific dataset being employed.</p>
<p>Given an adequate number of hidden neurons, networks with a single hidden layer can implement arbitrary decision boundaries and any higher number of hidden layers makes training times become very long.</p>
<p>The activation function in both hidden and output layers is the Sigmoid activation (it’s possible to apply ReLU activation function to the nodes in the hidden layer). Implementation details found in the file <code>ActivationFunctions.py</code>.</p>
<p>More details on the implementation of the ANN can be found in the header of the file <code>ANN.py</code>.</p>
<h3 id="note">Note</h3>
<p>The default number of <code>epochs</code> for stochastic gradient descent (SGD) and the <code>learning_rate</code> are set to <code>5</code> (to get end results fast) and <code>0.01</code>, respectively.</p>
<h2 id="binary-classification">Binary Classification</h2>
<p>The output neuron is the conditional probability of the <img src="https://latex.codecogs.com/gif.latex?+" alt="Equation"> class given the input, i.e., <img src="https://latex.codecogs.com/gif.latex?%5Cmathbb%7BP%7D%5B+%7C%5Cmathbf%7Bx%7D%5D" alt="Equation">. Given a threshold value <img src="https://latex.codecogs.com/gif.latex?0%3C%5Ctheta%3C1" alt="Equation">, a test example is classified as <img src="https://latex.codecogs.com/gif.latex?+" alt="Equation"> if <img src="https://latex.codecogs.com/gif.latex?%5Cmathbb%7BP%7D%5B+%7C%5Cmathbf%7Bx%7D%5D%3E%5Ctheta" alt="Equation"> and as <img src="https://latex.codecogs.com/gif.latex?-" alt="Equation"> otherwise.</p>
<p>Details on the implementation of cross-entropy loss function found in the file <code>LossFunction.py</code>.</p>
<h2 id="how-to">How-to</h2>
<h3 id="packages-and-modules-used">Packages and Modules Used</h3>
<pre class=" language-bash"><code class="prism  language-bash">numpy
pandas
matplotlib.pyplot
os
</code></pre>
<p>Use the package manager <code>pip</code> to install the necessary libraries.</p>
<h3 id="usage">Usage</h3>
<p>To run the submission, run the script <code>main.py</code> with every other file in the same directory.</p>
<p>The outputs of this submission are performance evaluation curves relevant to the task of binary classification. Details found in the file <code>EvaluationCurves.py</code>.</p>
<h4 id="note-1">Note</h4>
<p>The code tag <code># OPT</code> denotes <em>optional</em> lines or blocks of code that can be uncommented for modification of the parameters of the network, storage of auxiliary data, e.g., train and test data sets, and/or printing information-only statements.</p>
<p>Please don’t hesitate to contact me if you have any questions.<br>
<a href="mailto:Fakhteh.Saadatniaki@tufts.edu">Fakhteh.Saadatniaki@tufts.edu</a></p>
<p>Happy coding!</p>
</div>
</body>

</html>
